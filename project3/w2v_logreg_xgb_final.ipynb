{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Word2Vec + Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, VarianceThreshold, f_classif)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "\n",
    "from data_io import read_data\n",
    "from utils import label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "texts_train, labels_train = read_data(mode='train')\n",
    "y_train = [label_map[label] for label in labels_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "texts_val, labels_val = read_data(mode='val')\n",
    "y_val = [label_map[label] for label in labels_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "texts_test, labels_test = read_data(mode='test')\n",
    "y_test = [label_map[label] for label in labels_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that takes care of basic preprocessing steps such as lowercasing, stop-words removal, replacing digits and punctuation removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "translator = str.maketrans(\n",
    "    '', '', punctuation)\n",
    "stemmer = SnowballStemmer('english')\n",
    "stoplist = set(stopwords.words('english'))\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "def normalize(doc, stem=False):\n",
    "    \"\"\"\n",
    "    Input doc and return clean list of tokens\n",
    "    \"\"\"\n",
    "    doc = doc.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    lower = doc.lower()\n",
    "    doc = lower.translate(translator)\n",
    "    doc = doc.split()\n",
    "    doc = [w for w in doc if w not in stoplist]\n",
    "    doc = [w if not w.isdigit() else '#' for w in doc]\n",
    "    if stem:\n",
    "        doc = [stemmer.stem(w) for w in doc]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normalized_text_train = [normalize(doc) for doc in texts_train]\n",
    "normalized_text_val = [normalize(doc) for doc in texts_val]\n",
    "normalized_text_test = [normalize(doc) for doc in texts_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim.models\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Hyperparameter Tuning Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code can be used to tune the word2vec model for optimal performance of the XGBoost and/or logistic regression model. The data is first created accoring to the w2v parameterization and then stored for later use.\n",
    "\n",
    "WARNING: long runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_w2v = False \n",
    "\n",
    "if tune_w2v:\n",
    "    \n",
    "    # define directory to store embedded data\n",
    "    embd_dir = 'data/embeddings'\n",
    "    \n",
    "    # define parameters to search over\n",
    "    gridsearch_params = [(epochs, dim, min_words) \n",
    "                         for epochs in [5,10] \n",
    "                         for dim in [50,100,200] \n",
    "                         for min_words in [5,25,50]\n",
    "                        ]\n",
    "    \n",
    "    for epochs, dim, min_words in gridsearch_params:\n",
    "        model = gensim.models.Word2Vec(sentences = normalized_text_train,   # list of tokenized sentences\n",
    "                                       workers = 8,                         # Number of threads to run in parallel\n",
    "                                       iter = epochs,                       # Number of epochs\n",
    "                                       size = dim,                          # Word vector dimensionality     \n",
    "                                       min_count =  min_words               # Minimum word count  \n",
    "                                       )\n",
    "        \n",
    "        # get embedding from trained w2v model\n",
    "        Xtrain,ytrain = get_embedding(model, normalized_text_train, y_train)\n",
    "        Xval,yval = get_embedding(model, normalized_text_val, y_val)\n",
    "    \n",
    "        # save data for later use\n",
    "        np.savez_compressed(embd_dir + \"/embed_Xtrain_dim\" + str(dim) + \"_epochs\" + str(epochs) + \"_min_words\"+ str(min_words) + \".npz\", Xtrain)\n",
    "        np.savez_compressed(embd_dir + \"/embed_ytrain_dim\" + str(dim) + \"_epochs\" + str(epochs) + \"_min_words\"+ str(min_words) + \".npz\", ytrain)\n",
    "        \n",
    "        np.savez_compressed(embd_dir + \"/embed_ytrain_dim\" + str(dim) + \"_epochs\" + str(epochs) + \"_min_words\"+ str(min_words) + \".npz\", Xval)\n",
    "        np.savez_compressed(embd_dir + \"/embed_yeval_dim\" + str(dim) + \"_epochs\" + str(epochs) + \"_min_words\"+ str(min_words) + \".npz\", yval)\n",
    "\n",
    "    \n",
    "    # XGBoost Parameters\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'max_depth': 5,\n",
    "        'num_class': 5,\n",
    "        'eval_metric': ['merror', 'mlogloss'],\n",
    "        }\n",
    "        \n",
    "    res = []\n",
    "    for epochs, dim, min_words in gridsearch_params:\n",
    "        print(\"epochs={}, dim={}, min_words={} \".format(\n",
    "                                 epochs,\n",
    "                                 dim,\n",
    "                                 min_words))\n",
    "        # Load data\n",
    "        Xtrain = np.load(embd_dir + \"/embed_Xtrain_dim\" + str(dim) + \"_epochs\" + str(epochs) + \"_min_words\"+ str(min_words) + \".npz\")['arr_0']\n",
    "        Xval   = np.load(embd_dir + \"/embed_Xeval_dim\" + str(dim) + \"_epochs\" + str(epochs) + \"_min_words\"+ str(min_words) + \".npz\")['arr_0']\n",
    "        ytrain = np.load(embd_dir + \"/embed_ytrain_dim\" + str(dim) + \"_epochs\" + str(epochs) + \"_min_words\"+ str(min_words) + \".npz\")['arr_0']\n",
    "        yval   = np.load(embd_dir + \"/embed_yeval_dim\" + str(dim) + \"_epochs\" + str(epochs) + \"_min_words\"+ str(min_words) + \".npz\")['arr_0']\n",
    "        \n",
    "        # Fit Logistic Regression\n",
    "        lr = LogisticRegression(max_iter=1000)\n",
    "        lr.fit(Xtrain, ytrain)\n",
    "        \n",
    "        # Compute Score\n",
    "        lr.score(Xval, yval)\n",
    "        y_preds_val = lr.predict(Xval)\n",
    "        f1_lr = f1_score(yval, y_preds_val, average='micro')\n",
    "        print('LogReg f1 score micro : {}',f1_lr)\n",
    "              \n",
    "        # XGBoost \n",
    "        #  Data\n",
    "        dtrain = xgb.DMatrix(Xtrain, ytrain)\n",
    "        dval = xgb.DMatrix(Xval, yval)\n",
    "              \n",
    "        evallist = [(dtrain, 'train'), (dval, 'eval')]\n",
    "        \n",
    "        num_round = 50\n",
    "        bst = xgb.train(\n",
    "                params=params,  \n",
    "                dtrain=dtrain, \n",
    "                num_boost_round=num_round, \n",
    "                evals=evallist,\n",
    "                early_stopping_rounds=10,\n",
    "                verbose_eval=False)\n",
    "        \n",
    "        pred = bst.predict(dval)\n",
    "        f1_xgb = f1_score(yval, pred, average='micro')\n",
    "        print('XGBoost f1 score micro : {}',f1_xgb)\n",
    "        \n",
    "        res.append([epochs,dim,min_words,f1_lr,f1_xgb])\n",
    "      \n",
    "    # save results\n",
    "    np.savez_compressed(embd_dir +\"res.npz\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran hyperparamter tuning over the number of epochs, the dimensionality of the embedding and the minimum count of a word to be considered. Below are the parameters that gave the best results for the logistic regression and the xgboost consideres further down in the scirpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "dim = 300\n",
    "min_words = 25\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences = normalized_text_train,   # list of tokenized sentences\n",
    "                               workers = 8,                         # Number of threads to run in parallel\n",
    "                               iter = epochs,                       # Number of epochs\n",
    "                               size = dim,                          # Word vector dimensionality     \n",
    "                               min_count =  min_words               # Minimum word count  \n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to convert the words into a vector embedding. All the sentences that contain only words that were not learned by the Word2Vec model (e.g. due to a too small word frequency) are discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model,data,label):\n",
    "    x = []\n",
    "    nan_indices = []\n",
    "    for index in range(len(data)):\n",
    "        # remove out-of-vocabulary words\n",
    "        doc = [word for word in data[index] if word in model.wv.vocab.keys()]\n",
    "        if not doc:\n",
    "            # append zero vector\n",
    "            x.append(np.zeros(dim))\n",
    "\n",
    "        else:\n",
    "            # append the vector for each document\n",
    "            x.append(np.mean(model[doc], axis=0))\n",
    "        \n",
    "    X = np.array(x)\n",
    "    y = label\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/jmeirer/miniconda3/envs/mlhc_env/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain = get_embedding(model, normalized_text_train, y_train)\n",
    "Xval,yval = get_embedding(model, normalized_text_val, y_val)\n",
    "Xtest,ytest = get_embedding(model, normalized_text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check performance measured by micro F1-score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Score\n",
    "lr.score(Xtest, ytest)\n",
    "y_preds = lr.predict(Xtest)\n",
    "f1_lr = f1_score(ytest, y_preds, average='micro')\n",
    "print('LogReg f1 score micro : {}',f1_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'max_depth': 8,\n",
    "    'num_class': 5,\n",
    "    'eval_metric': ['merror', 'mlogloss'],\n",
    "    #'tree_method': 'gpu_hist'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "dtrain = xgb.DMatrix(Xtrain, ytrain)\n",
    "dval = xgb.DMatrix(Xval, yval)\n",
    "dtest = xgb.DMatrix(Xtest, ytest)\n",
    "      \n",
    "evallist = [(dtrain, 'train'), (dval, 'eval')]\n",
    "\n",
    "num_round = 100\n",
    "bst = xgb.train(\n",
    "        params=params,  \n",
    "        dtrain=dtrain, \n",
    "        num_boost_round=num_round, \n",
    "        evals=evallist,\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=True)\n",
    "\n",
    "pred = bst.predict(dtest)\n",
    "f1_xgb = f1_score(ytest, pred, average='micro')\n",
    "print('XGBoost f1 score micro : {}',f1_xgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "89a762d96eda99f70e99a85e9393056dd8a0d97e11de8075eba6ce85f6e9ad2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}