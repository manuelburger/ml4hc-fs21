{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b956fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, TfidfVectorizer)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, VarianceThreshold, f_classif)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, make_scorer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Custom\n",
    "from data_io import read_data\n",
    "from utils import label_map, normalize\n",
    "\n",
    "# Tensorflow BERT\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a857ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n"
     ]
    }
   ],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2297f4d8",
   "metadata": {},
   "source": [
    "### Load W&B\n",
    "Use Weights and Biases (http://wandb.ai) for monitoring (account required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b5e99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.login()\n",
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"bert.ipynb\"\n",
    "# wandb.init(project='mlhc-bert', entity='burgerm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07cc884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_config = {\n",
    "#   \"name\" : \"mlhc-bert-sweep-trainable-test\",\n",
    "#   \"project\" : \"mlhc-bert\",\n",
    "#   \"method\" : \"grid\",\n",
    "#   \"entity\": \"mlhc-bert\",\n",
    "#   \"parameters\" : {\n",
    "#     \"epochs\" : {\n",
    "#       \"values\" : [1, 5]\n",
    "#     },\n",
    "#     \"trainable\" :{\n",
    "#       \"values\" : [True]\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# wandb_sweep_id = wandb.sweep(sweep_config, project='mlhc-bert', entity='burgerm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fbca6e",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf36987",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, labels_train = read_data(mode='train')\n",
    "y_train_full = np.asarray([label_map[label] for label in labels_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfccc0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_val, labels_val = read_data(mode='val')\n",
    "y_val_full = np.asarray([label_map[label] for label in labels_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "830a0ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_test, labels_test = read_data(mode='test')\n",
    "y_test_full = np.asarray([label_map[label] for label in labels_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c1ff1-f52f-4396-bc88-4affc3f7db8b",
   "metadata": {},
   "source": [
    "#### Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ae0ff09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 11.245473819074784, 2: 3.061034949473142, 1: 6.510950387679048, 4: 2.8865257852639603, 3: 11.853425222801594}\n"
     ]
    }
   ],
   "source": [
    "N = len(y_train_full)\n",
    "count = Counter(y_train_full)\n",
    "class_weights = {cl: 1/(count[cl] / N) for cl in count}\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eda46f",
   "metadata": {},
   "source": [
    "### BERT\n",
    "Use pretrained BERT language models from Tensorflow Hub, specifically use model pretrained on the Pubmed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d00938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/google/experts/bert/pubmed/2\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "# bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2' \n",
    "bert_model_name = 'experts_pubmed' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2'\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60997956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained BERT model\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd69b45e",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1559a220-27bc-4b54-8ce6-7b1a120b1ea7",
   "metadata": {},
   "source": [
    "#### One-Hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa4f414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4051c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_oh = labelencoder.fit_transform(y_train_full)\n",
    "y_val_oh = labelencoder.fit_transform(y_val_full)\n",
    "y_test_oh = labelencoder.fit_transform(y_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41f4ac29-ce70-470a-b56b-0ce4bc9d1f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use raw input sentences, we will use the tensorflow BERT preprocessor\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(texts_train, dtype=tf.string), tf.convert_to_tensor(y_train_oh, dtype=tf.int32)))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(texts_val, dtype=tf.string), tf.convert_to_tensor(y_val_oh, dtype=tf.int32)))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(texts_test, dtype=tf.string), tf.convert_to_tensor(y_test_oh, dtype=tf.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01c49a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data\n",
    "# for text_batch, label_batch in train_ds.take(2):\n",
    "#     print(f'Review: {text_batch.numpy()}')\n",
    "#     label = label_batch.numpy()\n",
    "#     print(f'Label : {label}')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b952cd",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c86bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained BERT model from Tensorflow Hub\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eae60b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeline with preprocessor, BERT language model, dense classification head\n",
    "def build_classifier_model(train_transformer=False):\n",
    "    \n",
    "    # Input Layer\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    \n",
    "    # Preprocessing with preprocessor\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    \n",
    "    # Pass through pretrained BERT model\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=train_transformer, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    \n",
    "    # Attach dense classification head to the pooled output of the language model\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    net = tf.keras.layers.Dense(128)(net)\n",
    "    net = tf.keras.layers.Dense(128)(net)\n",
    "    net = tf.keras.layers.Dense(5, activation=\"softmax\", name='classifier')(net)\n",
    "    \n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bcb3194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model structure\n",
    "# tf.keras.utils.plot_model(classifier_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c90c071",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4685e326",
   "metadata": {},
   "source": [
    "#### W&B Sweep\n",
    "Use W&B Sweeps to tune hyperparamteres and directly store best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2ac5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    \n",
    "    # Initialize W&B\n",
    "    with wandb.init() as run:\n",
    "        \n",
    "        # Load W&B configuration for current run\n",
    "        config = wandb.config\n",
    "\n",
    "        # Loss function\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "        # Hyperparameter configuration\n",
    "        epochs = config[\"epochs\"]\n",
    "        batch_size = 128\n",
    "        steps_per_epoch = tf.data.experimental.cardinality(train_ds.batch(batch_size)).numpy()\n",
    "        num_train_steps = steps_per_epoch * epochs\n",
    "        num_warmup_steps = int(0.1*num_train_steps)\n",
    "        init_lr = 3e-5\n",
    "\n",
    "        # Setup multi-GPU training\n",
    "        strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"])\n",
    "        with strategy.scope():\n",
    "\n",
    "            # Optimizer, Adam with weight decay\n",
    "            optimizer = optimization.create_optimizer(\n",
    "                init_lr=init_lr,\n",
    "                num_train_steps=num_train_steps,\n",
    "                num_warmup_steps=num_warmup_steps,\n",
    "                optimizer_type='adamw'\n",
    "            )\n",
    "\n",
    "            # Load model\n",
    "            classifier_model = build_classifier_model(train_transformer=config[\"trainable\"])\n",
    "\n",
    "            # Compile model\n",
    "            classifier_model.compile(\n",
    "                optimizer=optimizer,\n",
    "                loss=loss,\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "         \n",
    "        # Store best loss model locally\n",
    "        save_best_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "            f'./model/bert_trainable_epochs{config[\"epochs\"]}_best_chkpt', monitor='val_loss', verbose=1, save_best_only=True,\n",
    "            save_weights_only=False, mode='min', save_freq='epoch'\n",
    "        )\n",
    "\n",
    "        # Run training\n",
    "        # The W&B Callback will report all scores to the online dashboard and store the\n",
    "        # final best validation loss model's weights for further usage\n",
    "        history = classifier_model.fit(\n",
    "            x=train_ds.batch(batch_size),\n",
    "            validation_data=val_ds.batch(batch_size),\n",
    "            epochs=config[\"epochs\"],\n",
    "            class_weight=class_weights,\n",
    "            callbacks=[WandbCallback(), save_best_cb]\n",
    "        )\n",
    "\n",
    "        # Predict score on Validation\n",
    "        y_proba_train = classifier_model.predict(train_ds.batch(64))\n",
    "        y_pred_train = np.argmax(y_proba_train, axis=1)\n",
    "        score = f1_score(y_train_full, y_pred_train, average=\"micro\")\n",
    "        print(f\"F1 Score on train: {score:.4f}\")\n",
    "\n",
    "        # Log score to W&B\n",
    "        wandb.log({\"train_f1\": score, \"trainable\": config[\"trainable\"], \"epochs\": config[\"epochs\"]})\n",
    "\n",
    "        # Predict score on Validation\n",
    "        y_proba_val = classifier_model.predict(val_ds.batch(64))\n",
    "        y_pred_val = np.argmax(y_proba_val, axis=1)\n",
    "        score = f1_score(y_val_full, y_pred_val, average=\"micro\")\n",
    "        print(f\"F1 Score on val: {score:.4f}\")\n",
    "        # Log score to W&B\n",
    "        wandb.log({\"val_f1\": score, \"trainable\": config[\"trainable\"], \"epochs\": config[\"epochs\"]})\n",
    "\n",
    "\n",
    "        # Predict score on Test\n",
    "        y_proba_test = classifier_model.predict(test_ds.batch(64))\n",
    "        y_pred_test = np.argmax(y_proba_test, axis=1)\n",
    "        score = f1_score(y_test_full, y_pred_test, average=\"micro\")\n",
    "        print(f\"F1 Score on Test: {score:.4f}\")\n",
    "        # Log score to W&B\n",
    "        wandb.log({\"test_f1\": score, \"trainable\": config[\"trainable\"], \"epochs\": config[\"epochs\"]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e2cf4-8a17-46a8-ae66-7a0a83042498",
   "metadata": {},
   "source": [
    "#### Run W&B Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d02265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run W&B Sweep\n",
    "# wandb.agent(wandb_sweep_id, function=training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
