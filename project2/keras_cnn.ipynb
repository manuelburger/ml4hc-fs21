{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd3b56c7",
   "metadata": {},
   "source": [
    "**Note:** The actual training was done in a Python script that corresponds to this notebook. The only difference is that the model gets saved in the end as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D, Layer, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, BatchNormalization, Activation, Add\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, confusion_matrix,\n",
    "    plot_precision_recall_curve, plot_roc_curve)\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, balanced_accuracy_score,\n",
    "    average_precision_score, precision_recall_curve,\n",
    "    confusion_matrix, auc, roc_curve,\n",
    "    plot_precision_recall_curve, plot_roc_curve,\n",
    "    recall_score, precision_score, auc)\n",
    "\n",
    "from cf_matrix import make_confusion_matrix\n",
    "from data_io import get_data, balance_out\n",
    "from data_representation import (\n",
    "    kmer_counts, one_hot_encoding, kmer_embeddings)\n",
    "from utils import (\n",
    "    get_class_distribution, binarize, plot_class_distribution)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-metadata",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "species = 'human'\n",
    "form = 'split'\n",
    "kmer_representation = '1-hot'  # either '1-hot' or 'dense'\n",
    "drop = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, y_train = get_data(species, form=form, mode='train', k=k, drop=drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde5eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_raw, y_val = get_data(species, form=form, mode='val', k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b41f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_raw, y_test = get_data(species, form=form, mode='test', k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b77d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[y_train < 0] = 0\n",
    "y_val[y_val < 0] = 0\n",
    "y_test[y_test < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-advocacy",
   "metadata": {},
   "source": [
    "### Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "if kmer_representation == '1-hot':\n",
    "    X_train = one_hot_encoding(X_train_raw, k=k, form='2d')\n",
    "    X_val = one_hot_encoding(X_val_raw, k=k,form='2d')\n",
    "    X_test = one_hot_encoding(X_test_raw, k=k,form='2d')\n",
    "elif kmer_representation == 'dense':\n",
    "    X_train = kmer_embeddings(X_train_raw, k=6, to_split=True)\n",
    "    X_val = kmer_embeddings(X_val_raw, k=6, to_split=True)\n",
    "    X_test = kmer_embeddings(X_test_raw, k=6, to_split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cy_train = to_categorical(y_train)\n",
    "cy_val = to_categorical(y_val)\n",
    "cy_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-province",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dbb4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = X_train.shape[1]\n",
    "N_CHANNELS = X_train.shape[2]\n",
    "\n",
    "INPUT_SHAPE = N_CHANNELS * SEQUENCE_LENGTH\n",
    "N_CLASSES = 2\n",
    "\n",
    "N_FILT = 32\n",
    "KERNEL_SIZE = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c13eb",
   "metadata": {},
   "source": [
    "### SpliceAI Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d74975",
   "metadata": {},
   "source": [
    "We also experimented with a model that is more faithful to the SpliceAI architecture, but we did not end up using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(Layer):\n",
    "    def __init__(self, n_filters, kernel_size, dilation_rate, **kwargs):\n",
    "        \n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        \n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "        \n",
    "        self.batch_norm1 = BatchNormalization()\n",
    "        \n",
    "        self.act1 = Activation('relu')\n",
    "        \n",
    "        self.conv_1 = Conv1D(\n",
    "            self.n_filters,\n",
    "            self.kernel_size,\n",
    "            dilation_rate=self.dilation_rate,\n",
    "            padding='causal')\n",
    "        \n",
    "        self.batch_norm2 = BatchNormalization()\n",
    "        \n",
    "        self.act2 = Activation('relu')\n",
    "        \n",
    "        self.conv_2 = Conv1D(\n",
    "            self.n_filters,\n",
    "            self.kernel_size,\n",
    "            dilation_rate=self.dilation_rate,\n",
    "            padding='causal')\n",
    "        \n",
    "        self.add1 = Add()\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        first_layer = x.copy()\n",
    "        \n",
    "        x = self.batch_norm1(x)\n",
    "        \n",
    "        x = self.act1(x)\n",
    "        \n",
    "        x = self.conv_1(x)\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        \n",
    "        x = self.act2(x)\n",
    "        \n",
    "        x = self.conv_2(x)\n",
    "        \n",
    "        x = self.add1([x, first_layer])\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "\n",
    "    \n",
    "X_input = Input(shape=(SEQUENCE_LENGTH, N_CHANNELS))\n",
    "\n",
    "X = Conv1D(\n",
    "    N_FILT, 1, dilation_rate=1, padding='causal')(X_input)\n",
    "\n",
    "X_r1 = Conv1D(\n",
    "    N_FILT, 1, dilation_rate=1, padding='causal')(X)\n",
    "\n",
    "X = Residual(N_FILT, KERNEL_SIZE, dilation_rate=1)(X)\n",
    "X = Residual(N_FILT, KERNEL_SIZE, dilation_rate=1)(X)\n",
    "X = Residual(N_FILT, KERNEL_SIZE, dilation_rate=1)(X)\n",
    "X = Residual(N_FILT, KERNEL_SIZE, dilation_rate=1)(X)\n",
    "\n",
    "X_r2 = Conv1D(\n",
    "    N_FILT, 1, dilation_rate=1, padding='causal')(X)\n",
    "\n",
    "X = Residual(N_FILT, KERNEL_SIZE, dilation_rate=4)(X)\n",
    "X = Residual(N_FILT, KERNEL_SIZE, dilation_rate=4)(X)\n",
    "X = Residual(N_FILT, KERNEL_SIZE, dilation_rate=4)(X)\n",
    "X = Residual(N_FILT, KERNEL_SIZE, dilation_rate=4)(X)\n",
    "\n",
    "X = Conv1D(\n",
    "    N_FILT, 1, dilation_rate=1, padding='causal')(X)\n",
    "\n",
    "X_r = Add()([X_r1, X_r2])\n",
    "\n",
    "X = Add()([X_r, X])\n",
    "\n",
    "X = Conv1D(3, 1, dilation_rate=1, padding='causal')(X)\n",
    "\n",
    "X = Flatten()(X)\n",
    "\n",
    "X = Dense(N_CLASSES, activation='softmax')(X)\n",
    "\n",
    "model = Model(inputs=X_input, outputs=X, name='SpliceAI-400')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fa28c",
   "metadata": {},
   "source": [
    "### Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(N_FILT, 1, dilation_rate=1, activation='relu', padding='causal', input_shape=(SEQUENCE_LENGTH, N_CHANNELS)))\n",
    "model.add(Conv1D(N_FILT, KERNEL_SIZE, dilation_rate=1, activation='relu', padding='causal'))\n",
    "model.add(Conv1D(N_FILT, KERNEL_SIZE, dilation_rate=1, activation='relu', padding='causal'))\n",
    "model.add(Conv1D(N_FILT, KERNEL_SIZE, dilation_rate=1, activation='relu', padding='causal'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(N_FILT, KERNEL_SIZE, dilation_rate=4, activation='relu', padding='causal'))\n",
    "model.add(Conv1D(N_FILT, KERNEL_SIZE, dilation_rate=4, activation='relu', padding='causal'))\n",
    "model.add(Conv1D(N_FILT, KERNEL_SIZE, dilation_rate=4, activation='relu', padding='causal'))\n",
    "model.add(Conv1D(N_FILT, KERNEL_SIZE, dilation_rate=4, activation='relu', padding='causal'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-mayor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam', \n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    cy_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight={0: 1, 1: 4},\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, cy_val),\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-steel",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test.astype)\n",
    "y_probs = y_pred[:, 1]\n",
    "y_pred_1d = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Recall = {recall_score(y_test, y_pred_1d)}.')\n",
    "print(f'Precision = {precision_score(y_test, y_pred_1d)}.')\n",
    "print(f'F1 score = {f1_score(y_test, y_pred_1d, average=\"macro\")}.')\n",
    "print(f'Balanced accuracy score = {balanced_accuracy_score(y_test, y_pred_1d)}.')\n",
    "print(f'AUROC = {roc_auc_score(y_test, y_probs, average=\"macro\")}.')\n",
    "print(f'AUPRC = {average_precision_score(y_test, y_probs, average=\"macro\")}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
