{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau)\n",
    "\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from data_io import get_data\n",
    "from utils import save_predictions\n",
    "from model_parts import get_attention_model, modify_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize result file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    {'model' : [], 'acc': [], 'f1-weighted': [], 'f1-macro': [], 'bacc': []})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MITBIH Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "X_train_mit, y_train_mit, X_test_mit, y_test_mit = get_data(dataset='mitbih')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### W&B setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.27<br/>\n                Syncing run <strong style=\"color:#cdcd00\">fluent-violet-67</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/anej/ML4HC-project4\" target=\"_blank\">https://wandb.ai/anej/ML4HC-project4</a><br/>\n                Run page: <a href=\"https://wandb.ai/anej/ML4HC-project4/runs/2z76ee1u\" target=\"_blank\">https://wandb.ai/anej/ML4HC-project4/runs/2z76ee1u</a><br/>\n                Run data is saved locally in <code>/cluster/scratch/asvete/project4/wandb/run-20210603_211006-2z76ee1u</code><br/><br/>\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='ML4HC-project4',\n",
    "    config={\n",
    "        'learning_rate': 2e-3,\n",
    "        'lr_decay_steps': 256,\n",
    "        'lr_decay_rate': 0.998,\n",
    "        'epochs': 8,\n",
    "        'batch_size': 32,\n",
    "        'num_filters': [32, 32, 256],\n",
    "        'num_blocks_list': [2, 2, 2],\n",
    "        'kernel_sizes': [5, 3, 3],\n",
    "        'loss_function': 'sparse_categorical_crossentropy',\n",
    "        'architecture': 'Attention',\n",
    "        'dataset': 'MITBIH',\n",
    "        'mode': 'training',\n",
    "        'ndim': 256,\n",
    "        'nheads': 3\n",
    "    })\n",
    "\n",
    "config_mit = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set-up model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 187, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 187, 1)       4           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 187, 32)      192         batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 187, 32)      128         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 94, 32)       3104        batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 94, 32)       128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_4 (TensorFl [(None, 94, 32)]     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_4 (TensorFlowOp [(None, 94, 32)]     0           batch_normalization_16[0][0]     \n",
      "                                                                 tf_op_layer_Sigmoid_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Identity_4 (TensorF [(None, 94, 32)]     0           tf_op_layer_Mul_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 94, 32)       1056        batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 94, 32)       3104        tf_op_layer_Identity_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 94, 32)       128         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 94, 32)       128         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 94, 32)       0           batch_normalization_18[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 94, 32)       128         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_5 (TensorFl [(None, 94, 32)]     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_5 (TensorFlowOp [(None, 94, 32)]     0           batch_normalization_19[0][0]     \n",
      "                                                                 tf_op_layer_Sigmoid_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Identity_5 (TensorF [(None, 94, 32)]     0           tf_op_layer_Mul_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 47, 256)      24832       tf_op_layer_Identity_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 47, 256)      1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_6 (TensorFl [(None, 47, 256)]    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_6 (TensorFlowOp [(None, 47, 256)]    0           batch_normalization_20[0][0]     \n",
      "                                                                 tf_op_layer_Sigmoid_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Identity_6 (TensorF [(None, 47, 256)]    0           tf_op_layer_Mul_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 47, 256)      8448        tf_op_layer_Identity_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 47, 256)      196864      tf_op_layer_Identity_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 47, 256)      1024        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 47, 256)      1024        conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 47, 256)      0           batch_normalization_22[0][0]     \n",
      "                                                                 batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 47, 256)      1024        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_7 (TensorFl [(None, 47, 256)]    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_7 (TensorFlowOp [(None, 47, 256)]    0           batch_normalization_23[0][0]     \n",
      "                                                                 tf_op_layer_Sigmoid_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Identity_7 (TensorF [(None, 47, 256)]    0           tf_op_layer_Mul_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 47, 256)      1024        tf_op_layer_Identity_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 47, 512)      1050624     batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_0 (AttentionLay (None, 512)          559         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_1 (AttentionLay (None, 512)          559         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_2 (AttentionLay (None, 512)          559         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 512)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 512)          0           attention_layer_0[0][0]          \n",
      "                                                                 attention_layer_1[0][0]          \n",
      "                                                                 attention_layer_2[0][0]          \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 512)          2048        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          131328      batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 256)          1024        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "final_dense_mitbih (Dense)      (None, 5)            1285        batch_normalization_27[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 1,498,166\n",
      "Trainable params: 1,493,236\n",
      "Non-trainable params: 4,930\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "original_model = get_attention_model(config_mit, nclass=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "file_path_mit = './models/attention/attention_mitbih.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    file_path_mit, monitor='val_f1-macro', verbose=1,\n",
    "    save_best_only=True, mode='max')\n",
    "early = EarlyStopping(\n",
    "    monitor='val_f1-macro', mode='max', patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(\n",
    "    monitor='val_acc', mode='max', patience=3, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_f1-macro did not improve from 0.90123\n",
      "\n",
      "Epoch 00005: val_f1-macro improved from 0.90123 to 0.91734, saving model to ./models/attention/attention_mitbih.h5\n",
      "\n",
      "Epoch 00006: val_f1-macro did not improve from 0.91734\n",
      "\n",
      "Epoch 00007: val_f1-macro did not improve from 0.91734\n",
      "\n",
      "Epoch 00008: val_f1-macro improved from 0.91734 to 0.92624, saving model to ./models/attention/attention_mitbih.h5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "593f400d15f6421ea20993477d40198f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d898977cd48f4c8bac21a7ef4a9044e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_f1-macro improved from -inf to 0.82220, saving model to ./models/attention/attention_mitbih.h5\n",
      "\n",
      "Epoch 00002: val_f1-macro did not improve from 0.82220\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-2e8082ccc36d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m history = original_model.fit(\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0mX_train_mit\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train_mit\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig_mit\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig_mit\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/wandb/integration/keras/keras.py\u001B[0m in \u001B[0;36mnew_v2\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    122\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mcbk\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcbks\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    123\u001B[0m                 \u001B[0mset_wandb_attrs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcbk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 124\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mold_v2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    125\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m     \u001B[0mtraining_arrays\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morig_fit_loop\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mold_arrays\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    106\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_method_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    107\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_in_multi_worker_mode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 108\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    109\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m     \u001B[0;31m# Running inside `run_distribute_coordinator` already.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1096\u001B[0m                 batch_size=batch_size):\n\u001B[1;32m   1097\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1098\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1099\u001B[0m               \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1100\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mtrain_function\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m    804\u001B[0m       \u001B[0;32mdef\u001B[0m \u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    805\u001B[0m         \u001B[0;34m\"\"\"Runs a training execution with one step.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 806\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mstep_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    807\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    808\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mstep_function\u001B[0;34m(model, iterator)\u001B[0m\n\u001B[1;32m    794\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    795\u001B[0m       \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 796\u001B[0;31m       \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistribute_strategy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrun_step\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    797\u001B[0m       outputs = reduce_per_replica(\n\u001B[1;32m    798\u001B[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m   1209\u001B[0m       fn = autograph.tf_convert(\n\u001B[1;32m   1210\u001B[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001B[0;32m-> 1211\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_extended\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall_for_each_replica\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1212\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1213\u001B[0m   \u001B[0;31m# TODO(b/151224785): Remove deprecated alias.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001B[0m in \u001B[0;36mcall_for_each_replica\u001B[0;34m(self, fn, args, kwargs)\u001B[0m\n\u001B[1;32m   2583\u001B[0m       \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2584\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_container_strategy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2585\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_for_each_replica\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2586\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2587\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_call_for_each_replica\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001B[0m in \u001B[0;36m_call_for_each_replica\u001B[0;34m(self, fn, args, kwargs)\u001B[0m\n\u001B[1;32m   2943\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_container_strategy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2944\u001B[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001B[0;32m-> 2945\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2946\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2947\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_reduce_to\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduce_op\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdestinations\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexperimental_hints\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    273\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    274\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mag_ctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mControlStatusCtx\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstatus\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mag_ctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mStatus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mUNSPECIFIED\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 275\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    276\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    277\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0minspect\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misfunction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0minspect\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mismethod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mrun_step\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m    787\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    788\u001B[0m       \u001B[0;32mdef\u001B[0m \u001B[0mrun_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 789\u001B[0;31m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    790\u001B[0m         \u001B[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    791\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol_dependencies\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_minimum_control_deps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mtrain_step\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    754\u001B[0m     \u001B[0;31m# The _minimize call does a few extra steps unnecessary in most cases,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    755\u001B[0m     \u001B[0;31m# such as loss scaling and gradient clipping.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 756\u001B[0;31m     _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n\u001B[0m\u001B[1;32m    757\u001B[0m               self.trainable_variables)\n\u001B[1;32m    758\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36m_minimize\u001B[0;34m(strategy, tape, optimizer, loss, trainable_variables)\u001B[0m\n\u001B[1;32m   2720\u001B[0m       \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_scaled_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2721\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2722\u001B[0;31m   \u001B[0mgradients\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgradient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrainable_variables\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2723\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2724\u001B[0m   \u001B[0;31m# Whether to aggregate gradients outside of optimizer. This requires support\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001B[0m in \u001B[0;36mgradient\u001B[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001B[0m\n\u001B[1;32m   1065\u001B[0m                           for x in nest.flatten(output_gradients)]\n\u001B[1;32m   1066\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1067\u001B[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001B[0m\u001B[1;32m   1068\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tape\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1069\u001B[0m         \u001B[0mflat_targets\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001B[0m in \u001B[0;36mimperative_grad\u001B[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001B[0m\n\u001B[1;32m     69\u001B[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001B[1;32m     70\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 71\u001B[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001B[0m\u001B[1;32m     72\u001B[0m       \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tape\u001B[0m\u001B[0;34m,\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m       \u001B[0mtarget\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001B[0m in \u001B[0;36m_gradient_function\u001B[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001B[0m\n\u001B[1;32m    160\u001B[0m       \u001B[0mgradient_name_scope\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mforward_pass_name_scope\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"/\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    161\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgradient_name_scope\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 162\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mgrad_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmock_op\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mout_grads\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    163\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    164\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mgrad_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmock_op\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mout_grads\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/ops/array_grad.py\u001B[0m in \u001B[0;36m_SqueezeGrad\u001B[0;34m(op, grad)\u001B[0m\n\u001B[1;32m    777\u001B[0m \u001B[0;34m@\u001B[0m\u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mRegisterGradient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Squeeze\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    778\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_SqueezeGrad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 779\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0m_ReshapeToInput\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    780\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/ops/array_grad.py\u001B[0m in \u001B[0;36m_ReshapeToInput\u001B[0;34m(op, grad)\u001B[0m\n\u001B[1;32m    766\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_ReshapeToInput\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    767\u001B[0m   \u001B[0;34m\"\"\"Reshapes the gradient to the shape of the original input.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 768\u001B[0;31m   return array_ops.reshape(\n\u001B[0m\u001B[1;32m    769\u001B[0m       _IndexedSlicesToTensorNoWarning(grad), array_ops.shape(op.inputs[0]))\n\u001B[1;32m    770\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    199\u001B[0m     \u001B[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    200\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 201\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    202\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m       \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001B[0m in \u001B[0;36mreshape\u001B[0;34m(tensor, shape, name)\u001B[0m\n\u001B[1;32m    194\u001B[0m   \"\"\"\n\u001B[1;32m    195\u001B[0m   \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgen_array_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m   \u001B[0mtensor_util\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmaybe_set_static_shape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/framework/tensor_util.py\u001B[0m in \u001B[0;36mmaybe_set_static_shape\u001B[0;34m(tensor, shape)\u001B[0m\n\u001B[1;32m   1061\u001B[0m     \u001B[0mshape\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mA\u001B[0m \u001B[0mshape\u001B[0m \u001B[0mtensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1062\u001B[0m   \"\"\"\n\u001B[0;32m-> 1063\u001B[0;31m   if (_ENABLE_MAYBE_SET_STATIC_SHAPE and not context.executing_eagerly() and\n\u001B[0m\u001B[1;32m   1064\u001B[0m       \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_default_graph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilding_function\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1065\u001B[0m       not tensor.shape.is_fully_defined() and is_tensor(shape)):\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/eager/context.py\u001B[0m in \u001B[0;36mexecuting_eagerly\u001B[0;34m()\u001B[0m\n\u001B[1;32m   1907\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdefault_execution_mode\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mEAGER_MODE\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1908\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1909\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1910\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1911\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cluster/apps/python/3.8.5_gpu_gcc630/x86_64/lib64/python3.8/site-packages/tensorflow/python/eager/context.py\u001B[0m in \u001B[0;36mexecuting_eagerly\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    789\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext_switches\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    790\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 791\u001B[0;31m   \u001B[0;32mdef\u001B[0m \u001B[0mexecuting_eagerly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    792\u001B[0m     \u001B[0;34m\"\"\"Returns True if current thread has eager executing enabled.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    793\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_thread_local_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_eager\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "history = original_model.fit(\n",
    "    X_train_mit, y_train_mit,\n",
    "    epochs=config_mit.epochs,\n",
    "    batch_size=config_mit.batch_size,\n",
    "    verbose=0,\n",
    "    callbacks=[\n",
    "        checkpoint, early, redonplat,\n",
    "        WandbCallback(), TqdmCallback(verbose=1)],\n",
    "    validation_split=0.1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 - 7s - loss: 0.0616 - acc: 0.9827 - f1-weighted: 0.9822 - f1-macro: 0.9008 - balanced_accuracy: 0.8917\n"
     ]
    }
   ],
   "source": [
    "loss, acc, f1_weighted, f1_macro, b_acc = original_model.evaluate(\n",
    "    X_test_mit, y_test_mit, batch_size=512, verbose=2)\n",
    "\n",
    "wandb.log({'Test Accuracy': acc})\n",
    "wandb.log({'Test F1 Weighted': f1_weighted})\n",
    "wandb.log({'Test F1 Macro': f1_macro})\n",
    "wandb.log({'Test Balanced Accuracy': b_acc})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 24092<br/>Program ended successfully."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value=' 17.40MB of 17.40MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76153441e8254247be4856771258b185"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/cluster/scratch/asvete/project4/wandb/run-20210603_195112-1i3r1qce/logs/debug.log</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/cluster/scratch/asvete/project4/wandb/run-20210603_195112-1i3r1qce/logs/debug-internal.log</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>epoch</td><td>7</td></tr><tr><td>loss</td><td>0.05383</td></tr><tr><td>acc</td><td>0.98386</td></tr><tr><td>f1-weighted</td><td>0.98232</td></tr><tr><td>f1-macro</td><td>0.91783</td></tr><tr><td>balanced_accuracy</td><td>0.93733</td></tr><tr><td>val_loss</td><td>0.05224</td></tr><tr><td>val_acc</td><td>0.98401</td></tr><tr><td>val_f1-weighted</td><td>0.98196</td></tr><tr><td>val_f1-macro</td><td>0.92624</td></tr><tr><td>val_balanced_accuracy</td><td>0.93944</td></tr><tr><td>lr</td><td><tensorflow.python.k...</td></tr><tr><td>_runtime</td><td>4722</td></tr><tr><td>_timestamp</td><td>1622747394</td></tr><tr><td>_step</td><td>11</td></tr><tr><td>best_val_loss</td><td>0.05224</td></tr><tr><td>best_epoch</td><td>7</td></tr><tr><td>Test Accuracy</td><td>0.98269</td></tr><tr><td>Test F1 Weighted</td><td>0.98219</td></tr><tr><td>Test F1 Macro</td><td>0.90081</td></tr><tr><td>Test Balanced Accuracy</td><td>0.89166</td></tr></table>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▁▁</td></tr><tr><td>acc</td><td>▁▅▆▇▇███</td></tr><tr><td>f1-weighted</td><td>▁▅▆▇▇███</td></tr><tr><td>f1-macro</td><td>▁▅▆▆▇███</td></tr><tr><td>balanced_accuracy</td><td>▁▅▆▇▇███</td></tr><tr><td>val_loss</td><td>█▇▃▃▂▄▃▁</td></tr><tr><td>val_acc</td><td>▁▂▇▆▇▅▆█</td></tr><tr><td>val_f1-weighted</td><td>▁▃▆▆▇▅▆█</td></tr><tr><td>val_f1-macro</td><td>▁▂▆▅▇▅▅█</td></tr><tr><td>val_balanced_accuracy</td><td>▁▄▆▅▇▄▇█</td></tr><tr><td>_runtime</td><td>▁▂▃▄▅▆▇█████</td></tr><tr><td>_timestamp</td><td>▁▂▃▄▅▆▇█████</td></tr><tr><td>_step</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>Test Accuracy</td><td>▁</td></tr><tr><td>Test F1 Weighted</td><td>▁</td></tr><tr><td>Test F1 Macro</td><td>▁</td></tr><tr><td>Test Balanced Accuracy</td><td>▁</td></tr></table><br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">effortless-haze-62</strong>: <a href=\"https://wandb.ai/anej/ML4HC-project4/runs/1i3r1qce\" target=\"_blank\">https://wandb.ai/anej/ML4HC-project4/runs/1i3r1qce</a><br/>\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.join()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.concat(\n",
    "    [results, pd.DataFrame([['MITBIH', acc, f1_weighted, f1_macro, b_acc]],\n",
    "                           columns=results.columns)],\n",
    "    axis=0).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predictions on the Test Set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_predictions(original_model, X_test_mit, 'attention_rnn', 'mit_test')\n",
    "save_predictions(original_model, X_train_mit, 'attention_rnn', 'mit_train')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PTBDB Transfer Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_pt, y_train_pt, X_test_pt, y_test_pt = get_data(dataset='ptbdb')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Frozen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### W&B setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='ML4HC-project4',\n",
    "    config={\n",
    "        'learning_rate': 2e-3,\n",
    "        'lr_decay_steps': 256,\n",
    "        'lr_decay_rate': 0.96,\n",
    "        'kernel_sizes': [5, 5, 3, 3],\n",
    "        'num_filters': [32, 32, 128, 128],\n",
    "        'epochs': 4,\n",
    "        'batch_size': 32,\n",
    "        'loss_function': 'sparse_categorical_crossentropy',\n",
    "        'architecture': 'Attention',\n",
    "        'dataset': 'PTBDB',\n",
    "        'mode': 'frozen',\n",
    "        'ndim': 256,\n",
    "        'nheads': 4\n",
    "    })\n",
    "\n",
    "config_pt_frozen = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set-up model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pretrained_model = get_attention_model(\n",
    "    nclass=5, prepare_model=False, config=config_pt_frozen)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path_pt_frozen = './models/attention/attention_pt_frozen.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    file_path_pt_frozen, monitor='val_f1-macro', verbose=1,\n",
    "    save_best_only=True, mode='max')\n",
    "early = EarlyStopping(\n",
    "    monitor='val_f1-macro', mode='max', patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(\n",
    "    monitor='val_acc', mode='max', patience=3, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Initialize model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pretrained_model.load_weights(file_path_mit)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Modify the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fine_tuned_model = modify_model(pretrained_model, config_pt_frozen)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fine_tuned_model.fit(\n",
    "    X_train_pt, y_train_pt,\n",
    "    epochs=config_pt_frozen.epochs,\n",
    "    batch_size=config_pt_frozen.batch_size,\n",
    "    verbose=0,\n",
    "    callbacks=[\n",
    "        checkpoint, early, redonplat,\n",
    "        WandbCallback(), TqdmCallback(verbose=1)],\n",
    "    validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss, acc, f1_weighted, f1_macro, b_acc = fine_tuned_model.evaluate(\n",
    "    X_test_pt, y_test_pt, batch_size=512, verbose=2)\n",
    "\n",
    "wandb.log({'Test Accuracy': acc})\n",
    "wandb.log({'Test F1 Weighted': f1_weighted})\n",
    "wandb.log({'Test F1 Macro': f1_macro})\n",
    "wandb.log({'Test Balanced Accuracy': b_acc})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.join()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.concat(\n",
    "    [results, pd.DataFrame([['PTBDB-frozen', acc, f1_weighted, f1_macro, b_acc]],\n",
    "                           columns=results.columns)],\n",
    "    axis=0).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predictions on the Test Set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_predictions(fine_tuned_model, X_test_mit, 'attention_rnn', 'ptbdb_frozen_test')\n",
    "save_predictions(fine_tuned_model, X_train_mit, 'attention_rnn', 'ptbdb_frozen_train')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Whole Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### W&B setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='ML4HC-project4',\n",
    "    config={\n",
    "        'learning_rate': 2e-3,\n",
    "        'lr_decay_steps': 256,\n",
    "        'lr_decay_rate': 0.96,\n",
    "        'kernel_sizes': [5, 5, 3, 3],\n",
    "        'num_filters': [32, 32, 128, 128],\n",
    "        'epochs': 4,\n",
    "        'batch_size': 32,\n",
    "        'loss_function': 'sparse_categorical_crossentropy',\n",
    "        'architecture': 'Attention',\n",
    "        'dataset': 'PTBDB',\n",
    "        'mode': 'whole',\n",
    "        'ndim': 256,\n",
    "        'nheads': 4\n",
    "    })\n",
    "\n",
    "config_pt_whole = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set-up model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pretrained_model = get_attention_model(\n",
    "    nclass=5, prepare_model=False, config=config_pt_whole)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path_pt_whole = './models/attention/attention_pt_whole.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    file_path_pt_whole, monitor='val_f1-macro', verbose=1,\n",
    "    save_best_only=True, mode='max')\n",
    "early = EarlyStopping(\n",
    "    monitor='val_f1-macro', mode='max', patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(\n",
    "    monitor='val_acc', mode='max', patience=3, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Initialize model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pretrained_model.load_weights(file_path_mit)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Modify the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fine_tuned_model = modify_model(pretrained_model, config_pt_whole)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fine_tuned_model.fit(\n",
    "    X_train_pt, y_train_pt,\n",
    "    epochs=config_pt_whole.epochs,\n",
    "    batch_size=config_pt_whole.batch_size,\n",
    "    verbose=0,\n",
    "    callbacks=[\n",
    "        checkpoint, early, redonplat,\n",
    "        WandbCallback(), TqdmCallback(verbose=1)],\n",
    "    validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss, acc, f1_weighted, f1_macro, b_acc = fine_tuned_model.evaluate(\n",
    "    X_test_pt, y_test_pt, batch_size=512, verbose=2)\n",
    "\n",
    "wandb.log({'Test Accuracy': acc})\n",
    "wandb.log({'Test F1 Weighted': f1_weighted})\n",
    "wandb.log({'Test F1 Macro': f1_macro})\n",
    "wandb.log({'Test Balanced Accuracy': b_acc})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run.join()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.concat(\n",
    "    [results, pd.DataFrame([['PTBDB-whole', acc, f1_weighted, f1_macro, b_acc]],\n",
    "                           columns=results.columns)],\n",
    "    axis=0).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save all Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results.to_csv(\n",
    "    './output/attention_results_epochs{}_batch{}.csv'.format(\n",
    "    config_pt_frozen.epochs, config_pt_frozen.batch_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predictions on the Test Set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_predictions(fine_tuned_model, X_test_mit, 'attention_rnn', 'ptbdb_whole_test')\n",
    "save_predictions(fine_tuned_model, X_train_mit, 'attention_rnn', 'ptbdb_whole_train')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}