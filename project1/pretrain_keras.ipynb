{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load general libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "# Load plotting and images\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Load sklearn functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# For parallelization\n",
    "import dask\n",
    "\n",
    "# Tensorflow and training\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.backend import int_shape\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU support\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device: \\\n",
    "    {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom helper functions\n",
    "from utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom functions and network for training\n",
    "from custom_unet import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subplots(plot_imgs, i, depth=0):\n",
    "    f, axarr = plt.subplots(1,8, figsize=(15,15))\n",
    "\n",
    "    axarr[0].imshow(plot_imgs[i][:,:,depth])\n",
    "    axarr[1].imshow(plot_imgs[i+1][:,:,depth])\n",
    "    axarr[2].imshow(plot_imgs[i+2][:,:,depth])\n",
    "    axarr[3].imshow(plot_imgs[i+3][:,:,depth])\n",
    "    axarr[4].imshow(plot_imgs[i+4][:,:,depth])\n",
    "    axarr[5].imshow(plot_imgs[i+5][:,:,depth])\n",
    "    axarr[6].imshow(plot_imgs[i+6][:,:,depth])\n",
    "    axarr[7].imshow(plot_imgs[i+7][:,:,depth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots_depth(img, depth=3, offset=0):\n",
    "    f, axarr = plt.subplots(1, depth, figsize=(15,15))\n",
    "    \n",
    "    for i in range(depth):\n",
    "        axarr[i].imshow(img[:,:,offset+i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Pretrain on the dataset downloaded from [here](https://www.kaggle.com/polomarco/chest-ct-segmentation) and extract to directory `./transfer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set locations of extracted images\n",
    "image_dir = \"./transfer/images/images\"\n",
    "lbl_dir = \"./transfer/masks/masks\"\n",
    "\n",
    "# Set label map and images with missing segmentation masks\n",
    "label_map = {\"lung\": 0, \"heart\": 1, \"trachea\":2}\n",
    "no_mask_p = {\"ID00149637202232704462834\", \"ID00222637202259066229764\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the downloaded images and their segmentation masks\n",
    "# uses the above configured directories and takes one of the three labels:\n",
    "# {trachea, lung, hear} to load the corresponding segm. mask\n",
    "def get_transfer_images(label=\"trachea\"):\n",
    "    \n",
    "    # Sub function to load pictures for a single patient\n",
    "    def load_patient(patient):\n",
    "        \n",
    "        # Find all files for given patient\n",
    "        p_files = [file for file in files if patient in file]\n",
    "        p_file_tuples = [(file, int(file[file.find(\"_\", -10)+1:-4])) for file in p_files]\n",
    "        p_file_tuples = sorted(p_file_tuples, key=lambda x: x[1])\n",
    "        \n",
    "        img = []\n",
    "        lbl = []\n",
    "        # Iterate over files and reconstruct 3D image\n",
    "        for img_f, l in p_file_tuples:\n",
    "            \n",
    "            patient_id_end = img_f.find(\"_\", -10)\n",
    "            patient_id = img_f[:patient_id_end]\n",
    "            \n",
    "            img_l = np.asarray(Image.open(image_dir + \"/\" + img_f))\n",
    "            img.append(img_l)\n",
    "            \n",
    "            lbl_l = np.asarray(Image.open(f\"{lbl_dir}/{patient_id}_mask_{l}.jpg\"))\n",
    "            lbl_l = lbl_l[:,:,label_map[label]]\n",
    "            lbl.append(lbl_l)\n",
    "            \n",
    "        return np.moveaxis(np.array(img, dtype=np.float32), 0, -1), np.moveaxis(np.array(lbl, dtype=np.float32), 0, -1)\n",
    "    \n",
    "    \n",
    "    # Get all files in directory\n",
    "    files = [f for f in os.listdir(image_dir)]\n",
    "    \n",
    "    patients_set = set()\n",
    "    images = []\n",
    "    lbls = []\n",
    "    \n",
    "    # Extract patient ID from files\n",
    "    for file in files:\n",
    "        patient_id_end = file.find(\"_\", -10)\n",
    "        patient_id = file[:patient_id_end]\n",
    "        \n",
    "        patients_set.add(patient_id)\n",
    "    \n",
    "    # Load image for each found patient\n",
    "    for p in tqdm(patients_set):\n",
    "        \n",
    "        if p not in no_mask_p:\n",
    "            p_img, p_lbl = load_patient(p)\n",
    "            images.append(p_img)\n",
    "            lbls.append(p_lbl)\n",
    "       \n",
    "    print(f\"Imported {len(images)} form {len(patients_set)} patients\")\n",
    "    print(\"Shape img:\", images[0].shape, \" Shape lbl:\", lbls[0].shape)\n",
    "    return images, lbls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load images\n",
    "Load images with trachea segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, lbls = get_transfer_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct labels to only contain 0 and 1\n",
    "def lable_correction(l):\n",
    "    l[l > 0] = 1\n",
    "    return l\n",
    "\n",
    "dask_objs = []\n",
    "for lbl in tqdm(lbls):\n",
    "    dask_objs.append(dask.delayed(lable_correction)(lbl))\n",
    "    \n",
    "lbls = dask.compute(*dask_objs, njobs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_train, imgs_valid, lbls_train, lbls_valid = train_test_split(\n",
    "    imgs,\n",
    "    lbls,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Data\", len(imgs_train), len(imgs_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion for training\n",
    "Convert 3D images for training. Create a separate sample for each layer, where we consider 2 neighbors on each side\n",
    "on the input and 1 neighbor on each side on the output. Thus the model will learn a mapping from a 5 channel 3D slice\n",
    "to a 3 channel 3D slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEIGHBORS = 2\n",
    "OUTPUT_NEIGHBORS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_sep = convert_depth_to_imgs_keras(imgs_train, neighbors=NEIGHBORS)\n",
    "valid_imgs_sep = convert_depth_to_imgs_keras(imgs_valid, neighbors=NEIGHBORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lbls_sep = convert_depth_to_imgs_keras(lbls_train, neighbors=OUTPUT_NEIGHBORS)\n",
    "valid_lbls_sep = convert_depth_to_imgs_keras(lbls_valid, neighbors=OUTPUT_NEIGHBORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate on full patients and load into contiguous memory arrays\n",
    "valid_x = np.array(valid_imgs_sep, dtype=np.float32)\n",
    "valid_y = np.array(valid_lbls_sep, dtype=np.float32)\n",
    "\n",
    "# Save memory and delete unused variables\n",
    "del valid_imgs_sep\n",
    "del valid_lbls_sep\n",
    "\n",
    "print(\"Validation data:\", len(valid_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation\n",
    "Augment training data with flips, rotations and noise for more robust training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformations(img, lbl):\n",
    "    \n",
    "    # Flip left-right randomly\n",
    "    choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
    "    img = tf.cond(choice < 0.5, lambda: img, lambda: tf.image.flip_left_right(img))\n",
    "    lbl = tf.cond(choice < 0.5, lambda: lbl, lambda: tf.image.flip_left_right(lbl))\n",
    "    \n",
    "    # Flip up-down randomly\n",
    "    choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
    "    img = tf.cond(choice < 0.5, lambda: img, lambda: tf.image.flip_up_down(img))\n",
    "    lbl = tf.cond(choice < 0.5, lambda: lbl, lambda: tf.image.flip_up_down(lbl))\n",
    "    \n",
    "    # Rotate by random angle\n",
    "    angle = tf.random.uniform(shape=[], minval=0, maxval=360, dtype=tf.int32)\n",
    "    angle = tf.dtypes.cast(angle, tf.float32)   \n",
    "    img = tfa.image.rotate(img, angle)\n",
    "    lbl = tfa.image.rotate(lbl, angle)\n",
    "    \n",
    "    # Add noise to image\n",
    "    noise = tf.random.normal(shape=tf.shape(img), mean=1.0, stddev=0.05, dtype=tf.float32)\n",
    "    noise_img = tf.dtypes.cast(img, tf.float32) * noise\n",
    "    \n",
    "    return (noise_img, lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "# load only a subset of the training data\n",
    "MAX_SAMPLES = 6000\n",
    "\n",
    "# apply augmentations\n",
    "for x, y in tqdm(zip(train_imgs_sep[:MAX_SAMPLES], train_lbls_sep[:MAX_SAMPLES]), total=MAX_SAMPLES):\n",
    "    x_out, y_out = apply_transformations(x, y)\n",
    "    train_x.append(np.array(x_out))\n",
    "    train_y.append(np.array(y_out))\n",
    "\n",
    "# Load into contiguous memory arrays\n",
    "train_x = np.array(train_x, dtype=np.float32)\n",
    "train_y = np.array(train_y, dtype=np.float32)\n",
    "\n",
    "# Save memory\n",
    "del train_imgs_sep\n",
    "del train_lbls_sep\n",
    "\n",
    "print(\"Training data:\", len(train_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focal Tversky Loss\n",
    "Implementation and further resource references at [Kaggle Post](https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras\n",
    "ALPHA = 0.5    # False Positive Penalty\n",
    "BETA = 1.0     # False Negative Penalty\n",
    "GAMMA = 4.0    # Focus more on false predictions\n",
    "\n",
    "def FocalTverskyLoss(targets, inputs, alpha=ALPHA, beta=BETA, gamma=GAMMA, smooth=1e-6):\n",
    "    \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = K.flatten(inputs)\n",
    "        targets = K.flatten(targets)\n",
    "        \n",
    "        #True Positives, False Positives & False Negatives\n",
    "        TP = K.sum((inputs * targets))\n",
    "        FP = K.sum(((1-targets) * inputs))\n",
    "        FN = K.sum((targets * (1-inputs)))\n",
    "               \n",
    "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
    "        FocalTversky = K.pow((1 - Tversky), gamma)\n",
    "        \n",
    "        return FocalTversky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Train custom U-Net architecture from [Github: karolzak/keras-unet](https://github.com/karolzak/keras-unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network\n",
    "strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"])\n",
    "with strategy.scope():\n",
    "    \n",
    "    unet = custom_unet(\n",
    "        train_x[0].shape,\n",
    "        num_classes=OUTPUT_NEIGHBORS * 2 + 1, # Configure number of output channels\n",
    "        filters=64, # number of filters in the first convolutional block (increased by factor 2 with depth)\n",
    "        use_batch_norm=True, # use batch normalization\n",
    "        dropout=0.2,  # set to value to use dropout after initial conv block\n",
    "        dropout_change_per_layer=0.0, # keep dropout on each layer constant\n",
    "        dropout_type='spatial', # use spatial dropout i.e. drop entire filters\n",
    "        num_layers=4, # 4 convolutional blocks (original U-Net depth)\n",
    "        upsample_mode='deconv', # use transposed convolutions in the upsampling part of the network\n",
    "        use_dropout_on_upsampling=False # don't use dropout in the upsampling part of the network\n",
    "    )\n",
    "    \n",
    "    unet.compile(\n",
    "        optimizer=Adam(),\n",
    "        loss=FocalTverskyLoss,\n",
    "        metrics=[iou, iou_thresholded, tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    \n",
    "# unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint best validation loss model\n",
    "save_best_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    './model/best_checkpoint_transfer', monitor='val_loss', verbose=1, save_best_only=True,\n",
    "    save_weights_only=False, mode='min', save_freq='epoch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain on the trachea dataset for 40 epochs over the loaded subset of data\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 24\n",
    "\n",
    "history = unet.fit(\n",
    "    train_x, train_y,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    validation_data=(valid_x, valid_y),\n",
    "    callbacks=[PlotLossesKeras(), save_best_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store model\n",
    "Store pretrained model for further use on the actual colon cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./model\", exist_ok=True)\n",
    "unet.save('./model/pretrained_transfer_trachea_1', overwrite=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
